# ============================================================
# Maximum Entropy Inverse Reinforcement Learningï¼ˆMaxEnt IRLï¼‰
# ------------------------------------------------------------
# ç›®çš„ï¼š
#   æ•™å¸«è»Œè·¡ï¼ˆãƒ‡ãƒ¢ï¼‰ã‹ã‚‰ã€Œå ±é…¬é–¢æ•°ã€ã‚’æ¨å®šã™ã‚‹ã€‚
#   ã“ã“ã§ã¯å„çŠ¶æ…‹ s ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« Ï†(s) ã‚’ one-hot ã¨ã¿ãªã—ã€
#   å ±é…¬ã‚’ R(s) = Î¸^T Ï†(s) ã¨ã„ã†ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§è¿‘ä¼¼ã™ã‚‹ã€‚
#
# ç†è«–ï¼ˆã–ã£ãã‚Šï¼‰ï¼š
#   - MaxEnt IRL ã¯ã€Œãƒ‡ãƒ¢ã«æ•´åˆã—ã¤ã¤ã€ä¸è¦ãªä»®å®šã‚’æœ€å°åŒ–ï¼ˆæœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰ã€ã™ã‚‹
#     è»Œè·¡åˆ†å¸ƒ p(Ï„) âˆ exp(Î£_t R(s_t)) ã‚’ä»®å®šã™ã‚‹ã€‚
#   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸ ã®æœ€å°¤æ¨å®šï¼ˆå¯¾æ•°å°¤åº¦å‹¾é…ï¼‰ã§å¾—ã‚‰ã‚Œã‚‹æ›´æ–°å‰‡ã¯
#       âˆ‡_Î¸ L(Î¸) = ğ”¼_demo[Î£_t Ï†(s_t)] - ğ”¼_Ï€Î¸[Î£_t Ï†(s_t)]
#     ã™ãªã‚ã¡ã€æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡æœŸå¾…ã¨ã€æ¨å®šå ±é…¬ä¸‹ã§æœ€é©åŒ–ã•ã‚ŒãŸæ–¹ç­–ã®ç‰¹å¾´é‡æœŸå¾…ã®ã€Œå·®ã€ã€‚
#   - æœ¬å®Ÿè£…ã§ã¯ã€å³é …ã®æœŸå¾…ã‚’ã€Œæ¨å®šå ±é…¬ã§è¨ˆç®—ã—ãŸæ–¹ç­–ï¼ˆPolicy Iterationï¼‰ã€ã®
#     å‰å‘ãç¢ºç‡ä¼æ’­ã§è¿‘ä¼¼ã—ã€ãã®å¹³å‡ç‰¹å¾´ã¨ã®å·®ã§ Î¸ ã‚’å‹¾é…ä¸Šæ˜‡ï¼ˆlearning_rateï¼‰ã•ã›ã‚‹ã€‚
#
# å®Ÿè£…ã®è¦ç‚¹ï¼š
#   - planner: PolicyIterationPlanner ã‚’ç”¨ã„ã¦ã€ç¾åœ¨ã® R(s)=Î¸^T Ï†(s) ã§æ–¹ç­–æœ€é©åŒ–ã€‚
#   - calculate_expected_feature: æ•™å¸«è»Œè·¡ã‹ã‚‰ã®ã€ŒçŠ¶æ…‹å‡ºç¾å›æ•°ã€ã‚’å¹³å‡åŒ–ï¼ˆçµŒé¨“çš„æœŸå¾…ï¼‰ã€‚
#   - expected_features_under_policy: åˆæœŸåˆ†å¸ƒã‚’æ•™å¸«è»Œè·¡ã‹ã‚‰æ¨å®šã—ã€é·ç§»ç¢ºç‡ã‚’å‰å‘ãã«ä¼æ’­
#     ï¼ˆå‰²å¼•ã¯ã‹ã‘ãšã€é•·ã•æ–¹å‘å¹³å‡ã‚’ã¨ã‚‹ç°¡æ˜“è¿‘ä¼¼ï¼‰ã—ã¦ã€çŠ¶æ…‹å‘¨è¾ºåˆ†å¸ƒã®å¹³å‡ã‚’è¨ˆç®—ã€‚
#   - Î¸ ã®æ›´æ–°ï¼šÎ¸ â† Î¸ + Î± * ( Î¼_demo - Î¼_Ï€Î¸ )  ï¼ˆç·šå½¢å ±é…¬ãªã®ã§ãã®ã¾ã¾å·®ã‚’åŠ ç®—ï¼‰
# ============================================================

import numpy as np
from planner import PolicyIterationPlanner
from tqdm import tqdm


class MaxEntIRL:

    def __init__(self, env):
        """
        env: GridWorld äº’æ›ï¼ˆstates, state_to_feature, transit_func ç­‰ãŒå¿…è¦ï¼‰
        """
        self.env = env
        self.planner = PolicyIterationPlanner(
            env
        )  # æ¨å®šå ±é…¬ä¸‹ã§æœ€é©æ–¹ç­–ã‚’æ±‚ã‚ã‚‹ãŸã‚ã«ä½¿ç”¨

    def estimate(self, trajectories, epoch=20, learning_rate=0.01, gamma=0.9):
        """
        MaxEnt IRL ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã€‚
        trajectories: æ•™å¸«ã®çŠ¶æ…‹ç³»åˆ—ï¼ˆlist[list[state]]ï¼‰
        epoch       : Î¸ ã®æ›´æ–°å›æ•°
        learning_rate: å‹¾é…ä¸Šæ˜‡ã®ã‚¹ãƒ†ãƒƒãƒ—å¹…
        gamma       : æ–¹ç­–è©•ä¾¡ãƒ»æ”¹å–„æ™‚ï¼ˆPolicy Iterationï¼‰ã«ç”¨ã„ã‚‹å‰²å¼•ç‡

        æˆ»ã‚Šå€¤ï¼š
          æ¨å®šã•ã‚ŒãŸå ±é…¬ï¼ˆenv.shape ã«æ•´å½¢ã—ãŸ 2D é…åˆ—ï¼‰
        """
        # çŠ¶æ…‹ç‰¹å¾´è¡Œåˆ— Î¦ï¼ˆè¡Œï¼šçŠ¶æ…‹ã€åˆ—ï¼šç‰¹å¾´ï¼‰ã€‚GridWorld(one-hot)ãªã‚‰å˜ä½è¡Œåˆ—ã«ç›¸å½“
        state_features = np.vstack(
            [self.env.state_to_feature(s) for s in self.env.states]
        )
        # Î¸ ã‚’ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ï¼ˆæ¬¡å…ƒï¼ç‰¹å¾´æ•°ï¼‰
        theta = np.random.uniform(size=state_features.shape[1])

        # æ•™å¸«ã®ã€ŒçµŒé¨“çš„ã€ç‰¹å¾´æœŸå¾… Î¼_demoï¼ˆçŠ¶æ…‹å‡ºç¾é »åº¦ã®å¹³å‡ï¼‰
        teacher_features = self.calculate_expected_feature(trajectories)

        for _ in tqdm(range(epoch)):
            # 1) ç¾åœ¨ã® Î¸ ã‹ã‚‰çŠ¶æ…‹å ±é…¬ãƒ™ã‚¯ãƒˆãƒ« R(s)=Î¸^T Ï†(s) ã‚’ä½œã‚‹
            rewards = state_features.dot(theta.T)  # shape: (num_states,)

            # 2) æ¨å®šå ±é…¬ä¸‹ã§æ–¹ç­–ã‚’æœ€é©åŒ–ï¼ˆPolicy Iterationï¼‰
            #    planner ã¯ env ã® reward_func ã‚’å‚ç…§ã™ã‚‹ãŸã‚ã€å·®ã—æ›¿ãˆã‚‹
            self.planner.reward_func = lambda s: rewards[s]
            self.planner.plan(gamma=gamma)  # æ–¹ç­–æ”¹å–„â†’ self.planner.policy ã«åæ˜ 

            # 3) ãã®æ–¹ç­–ã®ä¸‹ã§ã®ã€Œç‰¹å¾´æœŸå¾…ã€ Î¼_Ï€ ã‚’è¿‘ä¼¼è¨ˆç®—
            #    æˆ»ã‚Šå€¤ã¯çŠ¶æ…‹è¾ºã‚Šã®å‘¨è¾ºåˆ†å¸ƒï¼ˆå¹³å‡ï¼‰ã€‚ã“ã‚Œã‚’ç‰¹å¾´ç©ºé–“ã«å†™åƒ
            features = self.expected_features_under_policy(
                self.planner.policy, trajectories
            )
            # Î¼_Ï€ ã‚’ç‰¹å¾´ç©ºé–“ã¸ï¼ˆone-hot ãªã‚‰ state_features ã®ç·šå½¢çµåˆã§ OKï¼‰
            mu_pi = features.dot(state_features)  # shape: (num_features,)

            # 4) Î¸ ã‚’å‹¾é…ä¸Šæ˜‡ï¼šÎ¸ â† Î¸ + Î± ( Î¼_demo - Î¼_Ï€ )
            update = teacher_features - mu_pi
            theta += learning_rate * update

        # å­¦ç¿’å¾Œã®å ±é…¬ãƒãƒƒãƒ—ã‚’è¿”ã™ï¼ˆæç”»ãªã©ã—ã‚„ã™ã„ã‚ˆã†ã« 2D ã¸æˆå½¢ï¼‰
        estimated = state_features.dot(theta.T)
        estimated = estimated.reshape(self.env.shape)
        return estimated

    def calculate_expected_feature(self, trajectories):
        """
        æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®ã€ŒçµŒé¨“çš„ã€ç‰¹å¾´æœŸå¾…ï¼ˆã“ã“ã§ã¯çŠ¶æ…‹å‡ºç¾é »åº¦ã®å¹³å‡ï¼‰ã‚’è¨ˆç®—ã€‚
        GridWorld + one-hot ç‰¹å¾´ã§ã¯ã€å˜ã«å„çŠ¶æ…‹ã®å‡ºç¾å›æ•°ã‚’å¹³æ»‘åŒ–ã—ãŸã‚‚ã®ã«ãªã‚‹ã€‚

        æˆ»ã‚Šå€¤:
          shape: (num_features,) ï¼ (num_states,) ã‚’æƒ³å®š
        """
        features = np.zeros(self.env.observation_space.n)
        for t in trajectories:
            for s in t:
                features[s] += 1

        features /= len(trajectories)  # è»Œè·¡æœ¬æ•°ã§å¹³å‡
        return features

    def expected_features_under_policy(self, policy, trajectories):
        """
        æ¨å®šå ±é…¬ä¸‹ã§å¾—ãŸæ–¹ç­–ã®ä¸‹ã«ãŠã‘ã‚‹ã€ŒçŠ¶æ…‹å‘¨è¾ºåˆ†å¸ƒã€ã®è¿‘ä¼¼ã€‚

        æ‰‹é †ï¼ˆç°¡æ˜“è¿‘ä¼¼ï¼‰ï¼š
          1) åˆæœŸçŠ¶æ…‹åˆ†å¸ƒ p(s_0) ã‚’æ•™å¸«è»Œè·¡ã®å…ˆé ­çŠ¶æ…‹ã‹ã‚‰æ¨å®š
          2) æ™‚åˆ» t=1..T-1 ã¾ã§ã€p(s_t) = Î£_{s_{t-1}} p(s_{t-1}) P(s_t|s_{t-1}, a(s_{t-1}))
             ã‚’å‰å‘ãã«ä¼æ’­
             - a(s) ã¯æœ€é©æ–¹ç­–ã®æ±ºå®šçš„è¡Œå‹•ï¼ˆplanner.actï¼‰ã§è¿‘ä¼¼
          3) å„ t ã® p(s_t) ã‚’å¹³å‡ï¼ˆtime-averageï¼‰ã—ã¦æœ€çµ‚çš„ãªçŠ¶æ…‹å‘¨è¾ºåˆ†å¸ƒã‚’è¿”ã™
             ï¼ˆçœŸã® MaxEnt ã§ã¯å‰²å¼•ä»˜ãã®æœŸå¾…ã‚„ã€ã‚½ãƒ•ãƒˆãªé·ç§»ç­‰ã‚’è€ƒæ…®ã™ã‚‹ãŒã€
               ã“ã“ã§ã¯ç°¡æ½”ãªè¿‘ä¼¼ï¼‰

        æ³¨æ„ï¼š
          - å¼•æ•° policy ã¯ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ä¸Šå—ã‘å–ã‚‹ãŒã€å®Ÿè£…ã§ã¯ self.planner.act ã‚’ç”¨ã„ã¦
            ã€Œæ±ºå®šçš„ã«ã€è¡Œå‹•ã‚’é¸æŠã—ã¦ã„ã‚‹ï¼ˆæ–¹ç­–ãŒ stochastic ãªå ´åˆã¯è¦æ”¹å–„ï¼‰ã€‚
          - ã•ã‚‰ã«å³å¯†ã«ã¯ã€MaxEnt IRL ã®æœŸå¾…ã¯ soft policy / soft value ã§æ±‚ã‚ã‚‹ã“ã¨ãŒå¤šã„ã€‚
            æœ¬ã‚³ãƒ¼ãƒ‰ã¯ã€Œæœ€é©æ±ºå®šæ–¹ç­– + å‰å‘ãä¼æ’­ã€ã¨ã„ã†è¿‘ä¼¼ã§ã‚ã‚‹ã€‚
        """
        t_size = len(trajectories)  # ã“ã“ã§ã¯ã€Œé•·ã• T â‰ˆ è»Œè·¡æœ¬æ•°ã€ã¨ã„ã†è¿‘ä¼¼
        states = self.env.states
        transition_probs = np.zeros((t_size, len(states)))  # å„æ™‚åˆ»ã®å‘¨è¾ºåˆ†å¸ƒ p_t(s)

        # 1) åˆæœŸçŠ¶æ…‹åˆ†å¸ƒã‚’æ•™å¸«è»Œè·¡ã‹ã‚‰æ¨å®š
        initial_state_probs = np.zeros(len(states))
        for t in trajectories:
            initial_state_probs[t[0]] += 1
        initial_state_probs /= len(trajectories)
        transition_probs[0] = initial_state_probs

        # 2) æ–¹ç­– a(s)=argmax_Ï€ ã®ä¸‹ã§å‰å‘ãã«åˆ†å¸ƒä¼æ’­
        for t in range(1, t_size):
            for prev_s in states:
                prev_prob = transition_probs[t - 1][prev_s]
                if prev_prob == 0:
                    continue
                # NOTE: policy å¼•æ•°ã¯ä½¿ã‚ãšã€planner.act ã‚’ä½¿ã£ã¦æ±ºå®šçš„ã«è¡Œå‹•ã‚’é¸æŠ
                a = self.planner.act(prev_s)
                probs = self.env.transit_func(prev_s, a)  # P(s'|s,a)
                for s in probs:
                    transition_probs[t][s] += prev_prob * probs[s]

        # 3) æ™‚é–“å¹³å‡ã§æ»‘ã‚‰ã‹ã«ï¼ˆ1/T * Î£_t p_t(s)ï¼‰
        total = np.mean(transition_probs, axis=0)
        return total


if __name__ == "__main__":

    def test_estimate():
        """
        ç°¡å˜ãª GridWorld ã§ MaxEnt IRL ã‚’è©¦ã™ãƒ‡ãƒ¢ã€‚
          1) çœŸã®å ±é…¬ï¼ˆã‚°ãƒªãƒƒãƒ‰ã®å€¤ï¼‰ã§ PolicyIteration ã‚’å›ã—ã€æ•™å¸«è»Œè·¡ã‚’åé›†
          2) ãã®è»Œè·¡ã ã‘ã‚’è¦‹ã¦å ±é…¬ã‚’æ¨å®šï¼ˆå­¦ç¿’ï¼‰
          3) æ¨å®šå ±é…¬ã‚’è¡¨ç¤ºãƒ»å¯è¦–åŒ–
        """
        from environment import GridWorldEnv

        env = GridWorldEnv(
            grid=[
                [0, 0, 0, 1],
                [0, 0, 0, 0],
                [0, -1, 0, 0],
                [0, 0, 0, 0],
            ]
        )

        # 1) æ•™å¸«æ–¹ç­–ã®ä½œæˆï¼ˆçœŸã®å ±é…¬ã§æœ€é©åŒ–ï¼‰
        teacher = PolicyIterationPlanner(env)
        teacher.plan()

        # 2) æ•™å¸«è»Œè·¡ã‚’åé›†
        trajectories = []
        print("Gather demonstrations of teacher.")
        for _ in range(20):
            s = env.reset()
            done = False
            steps = [s]
            while not done:
                a = teacher.act(s)
                n_s, r, done, _ = env.step(a)
                steps.append(n_s)
                s = n_s
            trajectories.append(steps)

        # 3) å ±é…¬ã‚’æ¨å®šï¼ˆMaxEnt IRLï¼‰
        print("Estimate reward.")
        irl = MaxEntIRL(env)
        rewards = irl.estimate(trajectories, epoch=100)
        print(rewards)
        env.plot_on_grid(rewards)

    test_estimate()
